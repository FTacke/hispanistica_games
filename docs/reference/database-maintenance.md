# CO.RA.PAN - Datenbank-Wartung & Updates (Legacy)

> **âš ï¸ ARCHIVED**: This document describes maintenance for the legacy `transcription.db`.
> The application has migrated to **BlackLab-based search**; `transcription.db` no longer exists.
> For current corpus maintenance, rebuild the BlackLab index using:
> - Export: `python scripts/blacklab/run_export.py`
> - Build: `.\\scripts\\build_blacklab_index.ps1`
>
> For auth database (`auth.db`) maintenance, use standard SQLite tools.

**Zielgruppe:** Entwickler, Administratoren  
**Voraussetzungen:** Python 3.12+, SQLite 3.35+

---

## ğŸ“‹ Ãœbersicht

Dieses Dokument beschreibt, wie die optimierte Datenbank gewartet, aktualisiert und neu erstellt wird.

---

## ğŸ”§ Datenbank neu erstellen

### Wann ist ein Rebuild nÃ¶tig?

- âœ… Neue Transkriptions-Dateien hinzugefÃ¼gt
- âœ… JSON-Struktur hat sich geÃ¤ndert
- âœ… Datenbank ist beschÃ¤digt
- âœ… Performance-Probleme trotz Indizes
- âŒ **NICHT nÃ¶tig** nur fÃ¼r Code-Ã„nderungen

### Schritt-fÃ¼r-Schritt Anleitung

#### 1. Vorbereitung
```bash
# Projekt-Root Ã¶ffnen
cd "C:\Users\Felix Tacke\OneDrive\00 - MARBURG\DH-PROJEKTE\CO.RA.PAN\CO.RA.PAN-WEB_new"

# Virtual Environment aktivieren (falls nÃ¶tig)
.\.venv\Scripts\activate
```

#### 2. Datenbank neu erstellen
```bash
cd LOKAL\database
python database_creation_v2.py
```

**Erwartete Ausgabe:**
```
ğŸ”§ CO.RA.PAN Database Builder v2.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‚ Backup erstellen...
âœ“ Backup gespeichert: backups/20251018_143022/transcription.db

ğŸ“Š JSON-Dateien parsen...
âœ“ 1,351,207 Tokens geladen (6m 32s)

ğŸ—„ï¸  Datenbank erstellen...
âœ“ Schema erstellt
âœ“ 1,351,207 Rows eingefÃ¼gt

ğŸ“ˆ Performance-Indizes erstellen...
  1/7  idx_tokens_text          ... âœ“ (2.1s)
  2/7  idx_tokens_lemma         ... âœ“ (1.9s)
  3/7  idx_tokens_country_code  ... âœ“ (1.2s)
  4/7  idx_tokens_speaker_type  ... âœ“ (1.3s)
  5/7  idx_tokens_mode          ... âœ“ (1.1s)
  6/7  idx_tokens_filename_id   ... âœ“ (3.2s)
  7/7  idx_tokens_token_id      ... âœ“ (3.3s)
âœ“ Alle Indizes erstellt (14.1s)

ğŸ¯ ANALYZE ausfÃ¼hren...
âœ“ Query-Optimizer-Statistiken aktualisiert (0.9s)

âœ“ Datenbank erfolgreich erstellt!
   GrÃ¶ÃŸe: 348.97 MB
   Tokens: 1,351,207
   Dauer: 7m 47s
```

#### 3. ÃœberprÃ¼fung
```bash
# ZurÃ¼ck zum Projekt-Root
cd ..\..

# Server starten
$env:FLASK_APP="src.app.main"
.\.venv\Scripts\python.exe -m flask run --host=127.0.0.1 --port=8000
```

**Test durchfÃ¼hren:**
1. Browser Ã¶ffnen: http://127.0.0.1:8000
2. Zur Corpus-Seite navigieren
3. Nach "casa" suchen
4. Ergebnis sollte < 0.1s erscheinen

---

## ğŸ—‚ï¸ Backup-Strategie

### Automatisches Backup

`database_creation_v2.py` erstellt **automatisch** ein Backup vor jedem Rebuild:

```
LOKAL/database/backups/
  â”œâ”€â”€ 20251018_135510/
  â”‚   â”œâ”€â”€ transcription.db      (348 MB)
  â”‚   â””â”€â”€ metadata.json
  â”œâ”€â”€ 20251018_143022/
  â”‚   â”œâ”€â”€ transcription.db
  â”‚   â””â”€â”€ metadata.json
  â””â”€â”€ ...
```

**metadata.json enthÃ¤lt:**
```json
{
  "timestamp": "2025-10-18T14:30:22",
  "original_size_mb": 348.97,
  "row_count": 1351207,
  "reason": "pre_rebuild"
}
```

### Manuelles Backup erstellen

```bash
cd data\db
copy transcription.db transcription_backup_$(Get-Date -Format "yyyyMMdd_HHmmss").db
```

### Backup wiederherstellen

```bash
cd data\db
copy ..\..â€‹LOKAL\database\backups\20251018_135510\transcription.db transcription.db
```

**âš ï¸ Achtung:** Server muss gestoppt sein (WAL-Modus)!

---

## ğŸ“Š Datenbank-Status Ã¼berprÃ¼fen

### SQLite CLI verwenden

```bash
cd data\db
sqlite3 transcription.db
```

**Wichtige Queries:**

```sql
-- 1. Tabellenstruktur
.schema tokens

-- 2. Indizes auflisten
PRAGMA index_list('tokens');

-- 3. Index-Details
PRAGMA index_info('idx_tokens_text');

-- 4. ANALYZE-Status
SELECT * FROM sqlite_stat1;

-- 5. DatenbankgrÃ¶ÃŸe
SELECT page_count * page_size / 1024.0 / 1024.0 AS size_mb 
FROM pragma_page_count(), pragma_page_size();

-- 6. Token-Anzahl
SELECT COUNT(*) FROM tokens;

-- 7. Indizes-GrÃ¶ÃŸe
SELECT name, (pgsize / 1024.0 / 1024.0) AS size_mb 
FROM dbstat 
WHERE name LIKE 'idx_%' 
GROUP BY name;

-- 8. Query-Performance testen
.timer ON
SELECT * FROM tokens WHERE text LIKE '%casa%' LIMIT 25;

-- 9. Query-Plan analysieren
EXPLAIN QUERY PLAN 
SELECT * FROM tokens WHERE country_code = 'ARG';
```

### Python-Script fÃ¼r Checks

```python
# check_db_health.py
import sqlite3
from pathlib import Path

DB_PATH = Path("data/db/transcription.db")

def check_db_health():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # 1. Indizes prÃ¼fen
    cursor.execute("PRAGMA index_list('tokens')")
    indices = cursor.fetchall()
    print(f"âœ“ {len(indices)} Indizes gefunden")
    
    # 2. ANALYZE-Status
    cursor.execute("SELECT COUNT(*) FROM sqlite_stat1")
    stats = cursor.fetchone()[0]
    print(f"âœ“ {stats} ANALYZE-Statistiken vorhanden")
    
    # 3. Row Count
    cursor.execute("SELECT COUNT(*) FROM tokens")
    rows = cursor.fetchone()[0]
    print(f"âœ“ {rows:,} Tokens in Datenbank")
    
    # 4. WAL-Modus
    cursor.execute("PRAGMA journal_mode")
    mode = cursor.fetchone()[0]
    print(f"âœ“ Journal-Modus: {mode}")
    
    conn.close()
    print("\nâœ… Datenbank-Status: OK")

if __name__ == "__main__":
    check_db_health()
```

**AusfÃ¼hren:**
```bash
python LOKAL\database\check_db_health.py
```

---

## ğŸ”„ Neue Daten hinzufÃ¼gen

### Szenario: Neue Aufnahme-Session

#### 1. JSON-Dateien vorbereiten

Neue Dateien in `LOKAL/JSON-roh/` ablegen:
```
LOKAL/JSON-roh/
  â”œâ”€â”€ 2025-10-20_MEX_CDMX.json
  â”œâ”€â”€ 2025-10-21_MEX_CDMX.json
  â””â”€â”€ ...
```

**JSON-Struktur prÃ¼fen:**
```json
{
  "tokens": [
    {
      "id": 1,
      "text": "casa",
      "lemma": "casa",
      "start": 1.234,
      "end": 1.567,
      // ... weitere Felder
    }
  ]
}
```

#### 2. Audio-Dateien kopieren

```bash
# Full MP3s
copy \\Quelle\*.mp3 media\mp3-full\MEX\

# Transkripte (falls vorhanden)
copy \\Quelle\*.json media\transcripts\MEX\
```

#### 3. Datenbank neu erstellen

```bash
cd LOKAL\database
python database_creation_v2.py
```

**â±ï¸ Erwartete Dauer:**
- 1.000.000 Tokens: ~6 Minuten
- 1.500.000 Tokens: ~9 Minuten
- 2.000.000 Tokens: ~12 Minuten

#### 4. Testen

```bash
cd ..\..
$env:FLASK_APP="src.app.main"
.\.venv\Scripts\python.exe -m flask run --port=8000
```

Neue Daten sollten sofort durchsuchbar sein!

---

## ğŸ› ï¸ Indizes manuell neu erstellen

**Wann nÃ¶tig?**
- Indizes beschÃ¤digt
- Performance-Degradation
- Nach groÃŸem UPDATE/DELETE

### Methode 1: REINDEX (schnell)

```sql
sqlite3 data/db/transcription.db
REINDEX;
ANALYZE;
.quit
```

**Dauer:** ~5 Sekunden

### Methode 2: Drop & Recreate (grÃ¼ndlich)

```sql
DROP INDEX IF EXISTS idx_tokens_text;
DROP INDEX IF EXISTS idx_tokens_lemma;
DROP INDEX IF EXISTS idx_tokens_country_code;
DROP INDEX IF EXISTS idx_tokens_speaker_type;
DROP INDEX IF EXISTS idx_tokens_mode;
DROP INDEX IF EXISTS idx_tokens_filename_id;
DROP INDEX IF EXISTS idx_tokens_token_id;

-- Neu erstellen
CREATE INDEX idx_tokens_text ON tokens(text);
CREATE INDEX idx_tokens_lemma ON tokens(lemma);
CREATE INDEX idx_tokens_country_code ON tokens(country_code);
CREATE INDEX idx_tokens_speaker_type ON tokens(speaker_type);
CREATE INDEX idx_tokens_mode ON tokens(mode);
CREATE INDEX idx_tokens_filename_id ON tokens(filename, id);
CREATE UNIQUE INDEX idx_tokens_token_id ON tokens(token_id);

ANALYZE;
```

**Dauer:** ~15 Sekunden

---

## ğŸš¨ Troubleshooting

### Problem: "Database is locked"

**Ursache:** WAL-Modus, Server lÃ¤uft noch

**LÃ¶sung:**
```powershell
# 1. Server stoppen
Get-Process -Name python | Where-Object { $_.Path -like "*CO.RA.PAN*" } | Stop-Process -Force

# 2. WAL-Dateien aufrÃ¤umen
cd data\db
del transcription.db-wal
del transcription.db-shm

# 3. Rebuild durchfÃ¼hren
cd ..\..\LOKAL\database
python database_creation_v2.py
```

### Problem: Query langsam trotz Indizes

**Diagnose:**
```sql
EXPLAIN QUERY PLAN SELECT * FROM tokens WHERE text = 'casa';
```

**Sollte zeigen:**
```
SEARCH TABLE tokens USING INDEX idx_tokens_text (text=?)
```

**Falls "SCAN TABLE tokens" erscheint:**
```sql
ANALYZE;
REINDEX;
```

### Problem: "No such table: sqlite_stat1"

**Ursache:** ANALYZE wurde nie ausgefÃ¼hrt

**LÃ¶sung:**
```sql
ANALYZE;
```

### Problem: Datenbank grÃ¶ÃŸer als erwartet

**Diagnose:**
```sql
VACUUM;
```

**Effekt:**
- Entfernt gelÃ¶schte Daten
- Defragmentiert Datei
- Kann GrÃ¶ÃŸe um 10-30% reduzieren

**âš ï¸ Achtung:** Kann bei groÃŸen DBs lange dauern (>10 Min)!

---

## ğŸ“ˆ Performance-Monitoring

### Query-Zeiten loggen (Production)

In `src/app/services/corpus_search.py` hinzufÃ¼gen:

```python
import time
import logging

def search_tokens(params: SearchParams) -> dict[str, object]:
    start_time = time.time()
    
    # ... existing code ...
    
    elapsed = time.time() - start_time
    if elapsed > 0.5:  # Log slow queries
        logging.warning(f"Slow query: {elapsed:.2f}s - {params.query}")
    
    return results
```

### Benchmark-Script

```python
# benchmark_queries.py
import time
import sqlite3

DB_PATH = "data/db/transcription.db"

queries = [
    ("HÃ¤ufiges Wort", "SELECT * FROM tokens WHERE text = 'de' LIMIT 25"),
    ("LIKE Query", "SELECT * FROM tokens WHERE text LIKE '%casa%' LIMIT 25"),
    ("Token-ID", "SELECT * FROM tokens WHERE token_id = 'ARG001'"),
    ("Filter", "SELECT * FROM tokens WHERE country_code = 'ARG' AND mode = 'libre' LIMIT 25"),
]

conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

print("ğŸ“Š Performance Benchmark\n")
for name, query in queries:
    start = time.time()
    cursor.execute(query)
    cursor.fetchall()
    elapsed = time.time() - start
    print(f"{name:20s}: {elapsed*1000:6.2f} ms")

conn.close()
```

**Erwartete Ausgabe:**
```
ğŸ“Š Performance Benchmark

HÃ¤ufiges Wort        :   1.23 ms
LIKE Query           :  83.45 ms
Token-ID             :   2.01 ms
Filter               :   5.67 ms
```

---

## ğŸ” Best Practices

### âœ… DO:
- Backup vor jedem Rebuild erstellen
- ANALYZE nach Index-Ã„nderungen ausfÃ¼hren
- WAL-Modus fÃ¼r Concurrency nutzen
- Query-Performance regelmÃ¤ÃŸig testen
- Indizes auf hÃ¤ufig gefilterten Spalten

### âŒ DON'T:
- Datenbank direkt editieren wÃ¤hrend Server lÃ¤uft
- VACUUM ohne Backup
- Indizes auf jede Spalte (Overhead!)
- Transaction ohne COMMIT laufen lassen
- Manuelle Schema-Ã„nderungen (immer via Script!)

---

## ğŸ“ Support

Bei Problemen:
1. Logs prÃ¼fen: `logs/application.log`
2. DB-Health-Check ausfÃ¼hren
3. Backup wiederherstellen falls nÃ¶tig
4. Rebuild als letztes Mittel

---

**Erstellt:** 18. Oktober 2025  
**Version:** 1.0  
**NÃ¤chste Review:** Bei Schema-Ã„nderungen
