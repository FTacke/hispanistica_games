---
title: "JSON-Annotation v2 Spezifikation"
status: active
owner: backend-team
updated: "2025-11-08"
tags: [annotation, json, nlp, spacy, specification, corpus]
links:
  - ../how-to/json-annotation-workflow.md
  - corpus-search-architecture.md
  - ../operations/database-creation.md
---

# JSON-Annotation v2 Spezifikation

Vollst√§ndige Spezifikation des CO.RA.PAN JSON-Annotations-Schemas Version 2.

---

## √úberblick

**Version:** `corapan-ann/v2`  
**Script:** `LOKAL/01 - Add New Transcriptions/02 annotate JSON/annotation_json_in_media_v2.py`  
**spaCy-Modell:** `es_dep_news_trf`

### Erweiterungen gegen√ºber v1

| Feature | v1 | v2 |
|---------|----|----|
| **Token-IDs** | ‚ùå Keine | ‚úÖ Stabil, hierarchisch |
| **Satz-/√Ñu√üerungs-IDs** | ‚ùå Keine | ‚úÖ Hierarchisch |
| **Zeitstempel** | ‚úÖ Sekunden (float) | ‚úÖ Millisekunden (int) |
| **Normalisierung** | ‚ùå Keine | ‚úÖ `norm` Feld |
| **Idempotenz** | ‚ö†Ô∏è Grob (pos vorhanden?) | ‚úÖ Text-Hash + Felder |
| **Metadaten** | ‚ùå Keine | ‚úÖ `ann_meta` Objekt |
| **Perfektformen** | ‚ö†Ô∏è String-basiert (head_text) | ‚úÖ Lemma-/morph-basiert |
| **Analytisches Futur** | ‚ö†Ô∏è Festes 3-Token-Fenster | ‚úÖ Flexibel mit Gap-Handling |
| **BlackLab-Export** | ‚ùå Nested in morph | ‚úÖ Flache Felder (past_type, future_type) |

---

## JSON-Struktur

### Top-Level Schema

```json
{
  "ann_meta": {
    "version": "corapan-ann/v2",
    "spacy_model": "es_dep_news_trf",
    "text_hash": "abc123...",
    "required": ["token_id", "sentence_id", ...],
    "timestamp": "2025-11-08T14:30:00+00:00"
  },
  "segments": [
    {
      "id": 0,
      "utt_start_ms": 1200,
      "utt_end_ms": 8500,
      "words": [...]
    }
  ]
}
```

---

## Metadaten-Objekt (`ann_meta`)

Pflichtfelder f√ºr Idempotenz und Versionskontrolle.

### Felder

| Feld | Typ | Beschreibung | Beispiel |
|------|-----|--------------|----------|
| `version` | String | Schema-Version | `"corapan-ann/v2"` |
| `spacy_model` | String | Verwendetes spaCy-Modell | `"es_dep_news_trf"` |
| `text_hash` | String | SHA1-Hash √ºber alle Token-Texte | `"a1b2c3d4..."` |
| `required` | Array | Liste der Pflicht-Token-Felder | `["token_id", ...]` |
| `timestamp` | String | ISO-8601 Zeitstempel (UTC) | `"2025-11-08T14:30:00+00:00"` |

### Idempotenz-Logik

Datei wird **√ºbersprungen** wenn:
1. `ann_meta.version == "corapan-ann/v2"`
2. `ann_meta.text_hash == aktueller_hash`
3. Alle `required` Felder in allen Tokens vorhanden

Sonst: **Neu annotieren**

---

## Segment-Felder

Pro √Ñu√üerung/Segment (entspricht Whisper-Segment).

### Pflichtfelder

| Feld | Typ | Beschreibung | Beispiel |
|------|-----|--------------|----------|
| `utt_start_ms` | Integer | Start der √Ñu√üerung in ms | `1200` |
| `utt_end_ms` | Integer | Ende der √Ñu√üerung in ms | `8500` |
| `words` | Array | Liste der Token-Objekte | `[...]` |

**Berechnung:**
```python
utt_start_ms = min(word.start_ms for word in words)
utt_end_ms = max(word.end_ms for word in words)
```

---

## Token-Felder

Pro Token (Wort) im Korpus.

### Pflichtfelder

| Feld | Typ | Beschreibung | Beispiel |
|------|-----|--------------|----------|
| **IDs** | | | |
| `token_id` | String | Eindeutige Token-ID | `"ARG_001:0:0:5"` |
| `sentence_id` | String | Satz-ID | `"ARG_001:0:s0"` |
| `utterance_id` | String | √Ñu√üerungs-ID | `"ARG_001:0"` |
| **Zeit** | | | |
| `start_ms` | Integer | Token-Start in ms | `1200` |
| `end_ms` | Integer | Token-Ende in ms | `1500` |
| **Text & Basis-Annotation** | | | |
| `text` | String | Original-Text | `"Est√°"` |
| `lemma` | String | Lemma (Grundform) | `"estar"` |
| `pos` | String | Part-of-Speech Tag | `"AUX"` |
| `dep` | String | Dependency-Relation | `"cop"` |
| `head_text` | String | Head-Token Text | `"bien"` |
| `morph` | Object | Morphologische Features | `{"Mood": "Ind", ...}` |
| **Normalisierung & Zeitformen** | | | |
| `norm` | String | Normalisierte Suchform | `"esta"` |
| `past_type` | String | Vergangenheitsform-Label | `"PerfectoCompuesto"` |
| `future_type` | String | Zukunftsform-Label | `"analyticalFuture"` |

### Optionale Felder

| Feld | Typ | Beschreibung | Beispiel |
|------|-----|--------------|----------|
| `start` | Float | Original-Start in Sekunden | `1.2` |
| `end` | Float | Original-Ende in Sekunden | `1.5` |
| `foreign` | String | Fremdwort-Flag | `"1"` |

---

## ID-Hierarchie

### Format

```
token_id      = "{file_id}:{utt_idx}:{sent_idx}:{token_idx}"
sentence_id   = "{file_id}:{utt_idx}:s{sent_idx}"
utterance_id  = "{file_id}:{utt_idx}"
file_id       = "{country}_{file_number}"
```

### Beispiel

**Datei:** `media/transcripts/ARG/001.json`  
**File-ID:** `ARG_001`

**Token 6 in Satz 1 der √Ñu√üerung 2:**
- `token_id`: `"ARG_001:2:1:6"`
- `sentence_id`: `"ARG_001:2:s1"`
- `utterance_id`: `"ARG_001:2"`

### Eigenschaften

- ‚úÖ **Deterministisch**: Gleiche Datei ‚Üí gleiche IDs
- ‚úÖ **Stabil**: IDs √§ndern sich nicht bei Re-Annotation (solange Text gleich)
- ‚úÖ **Hierarchisch**: Token ‚Üí Satz ‚Üí √Ñu√üerung ‚Üí Datei
- ‚úÖ **Sortierbar**: Lexikographisch sortiert = chronologisch

---

## Normalisierung (`norm`)

Deterministische Pipeline f√ºr akzent-/case-indifferente Suche.

### Algorithmus

```python
1. Unicode NFKD-Normalisierung
2. Entferne kombinierende Akzente au√üer Tilde (√± bleibt √±)
3. Lowercase
4. Entferne f√ºhrende/trailing Interpunktion (inkl. ¬ø¬°)
5. Whitespace komprimieren
```

### Beispiele

| Original | `norm` | Erkl√§rung |
|----------|--------|-----------|
| `"¬°Est√°!"` | `"esta"` | Akzent weg, lowercase, Interpunktion weg |
| `"a√±o"` | `"a√±o"` | Tilde bleibt! |
| `"M√©xico"` | `"mexico"` | Akzent weg |
| `"  caf√©  "` | `"cafe"` | Whitespace komprimiert, Akzent weg |
| `"¬øQu√©?"` | `"que"` | Akzent weg, Interpunktion weg |

### Verwendung

**Such-Query:** `"esta"` ‚Üí findet `"Est√°"`, `"est√°"`, `"EST√Å"`, `"¬°est√°!"`

**Datenbank:**
```sql
WHERE norm LIKE '%esta%'
```

---

## Vergangenheitsformen (Perfekt)

Robuste Erkennung via **lemma + morph** statt String-Listen.

### Strategie

1. **PerfectoSimple**: `Tense=Past` + `VerbForm=Fin` (nicht-AUX)
2. **Partizip + AUX haber**: Suche AUX mit `lemma="haber"` innerhalb ‚â§3 nicht-ignorierbarer Tokens
3. **Tense-Mapping**: Tense des AUX ‚Üí Perfektform-Label

### Erkannte Labels (`past_type`)

| Label | Beschreibung | Beispiel | AUX Tense |
|-------|--------------|----------|-----------|
| `PerfectoSimple` | Einfache Vergangenheit | `"cant√©"` | - |
| `PerfectoCompuesto` | Zusammengesetztes Perfekt | `"he cantado"` | `Pres` |
| `Pluscuamperfecto` | Plusquamperfekt | `"hab√≠a cantado"` | `Imp` |
| `FuturoPerfecto` | Futur II | `"habr√© cantado"` | `Fut` |
| `CondicionalPerfecto` | Konditional Perfekt | `"habr√≠a cantado"` | `Cond` |
| `OtroCompuesto` | Andere zusammengesetzte Form | - | (andere) |
| `PastOther` | Andere Vergangenheit | - | - |

### Gap-Handling

**Erlaubte Zwischentokens** (werden √ºbersprungen):
- **POS**: `PRON`, `ADV`, `PART`, `ADP`, `SCONJ`, `PUNCT`
- **Tokens**: `no`, `ya`, `a√∫n`, `todav√≠a`, `tambi√©n`, `solo`, `s√≥lo`

**Beispiele:**
```
"ya ha cantado"         ‚Üí PerfectoCompuesto (1 Gap: 'ya')
"no ha cantado a√∫n"     ‚Üí PerfectoCompuesto (2 Gaps: 'no', 'a√∫n')
"lo ha cantado"         ‚Üí PerfectoCompuesto (1 Gap: 'lo')
"hab√≠a ya cantado"      ‚Üí Pluscuamperfecto (1 Gap: 'ya')
```

### Exklusionen

‚ùå **Existential haber**: Partizip mit `lemma="haber"` ‚Üí **nicht** als Perfekt klassifizieren

```
"hubo lluvia"  ‚Üí PastOther (nicht PerfectoSimple auf AUX)
"hab√≠a gente"  ‚Üí Keine Perfekt-Label
```

---

## Zukunftsformen (Analytisches Futur)

Flexibles Fenster f√ºr `ir + a + Infinitiv`.

### Strategie

1. **Finde `ir`**: Token mit `lemma="ir"` und `POS in {AUX, VERB}`
2. **Finde `a`**: ADP innerhalb ‚â§3 nicht-ignorierbarer Tokens
3. **Finde Infinitiv**: VERB mit `VerbForm=Inf` innerhalb ‚â§3 nicht-ignorierbarer Tokens nach `a`
4. **Label nach Tense**: `Pres` ‚Üí `analyticalFuture`, `Imp` ‚Üí `analyticalFuture_past`

### Erkannte Labels (`future_type`)

| Label | Beschreibung | Beispiel | ir Tense |
|-------|--------------|----------|----------|
| `analyticalFuture` | Analytisches Futur (Pr√§sens) | `"voy a cantar"` | `Pres` |
| `analyticalFuture_past` | Analytisches Futur (Imperfekt) | `"iba a cantar"` | `Imp` |

### Gap-Handling

**Beispiele:**
```
"voy a cantar"          ‚Üí analyticalFuture
"no voy a cantar"       ‚Üí analyticalFuture (1 Gap: 'no')
"voy a cantar ya"       ‚Üí analyticalFuture
"iba a cantar"          ‚Üí analyticalFuture_past
"vamos a ir a Madrid"   ‚Üí analyticalFuture (nur 1. Infinitiv markiert)
```

### Exklusionen

‚ùå **`ir a` + Nomen**: Kein `future_type` Label

```
"voy a Madrid"     ‚Üí Kein Label (kein Infinitiv)
"ir a la tienda"   ‚Üí Kein Label
```

---

## Flache Felder f√ºr BlackLab

F√ºr einfachen Export und Indizierung.

### `past_type` & `future_type`

**Quelle:** Nested in `morph.Past_Tense_Type` und `morph.Future_Type`  
**Ziel:** Flache String-Felder im Token-Objekt

**Extraktion:**
```python
w["past_type"] = w.get("morph", {}).get("Past_Tense_Type", "")
w["future_type"] = w.get("morph", {}).get("Future_Type", "")
```

**Vorteile:**
- ‚úÖ Triviales Mapping f√ºr DB-Export
- ‚úÖ Direkte CQL-Abfragen in BlackLab
- ‚úÖ Keine Nested-Dict-Navigation n√∂tig

**Beispiel-CQL:**
```
[past_type="PerfectoCompuesto"]  # Alle Perfecto-Compuesto Formen
[future_type="analyticalFuture"] # Alle 'ir a + Inf' Formen
```

---

## Satz-Bildung

Algorithmus zur Unterteilung von √Ñu√üerungen in S√§tze.

### Regel

Token mit Satzende-Zeichen (`.`, `?`, `!`) markieren Satzgrenze.

### Beispiel

**√Ñu√üerung:** `"Hola. ¬øC√≥mo est√°s? Bien."`

**S√§tze:**
1. `["Hola", "."]` ‚Üí `sentence_id: "...:s0"`
2. `["¬ø", "C√≥mo", "est√°s", "?"]` ‚Üí `sentence_id: "...:s1"`
3. `["Bien", "."]` ‚Üí `sentence_id: "...:s2"`

### Kontext-Annotation

**spaCy-Parsing:** Satz-1 + Satz + Satz+1 (f√ºr bessere Dependency-Aufl√∂sung)

---

## Spezialf√§lle

### Foreign-W√∂rter

**Kennzeichnung:** `"foreign": "1"` im Token-Objekt

**Verhalten:**
- ‚úÖ IDs werden generiert
- ‚úÖ Zeit-Felder werden gesetzt
- ‚ùå **Keine spaCy-Annotation** (pos, lemma, etc. fehlen)

### Self-Correction (Abgebrochene W√∂rter)

**Muster:** Token endet mit `-` (z.B. `"tu-"`, `"est-,"`)

**Annotation:**
```json
{
  "text": "tu-",
  "pos": "self-correction",
  "lemma": "tu-",
  "dep": "",
  "head_text": "",
  "morph": {}
}
```

### Interjektionen

**Beispiel:** `"eeh"`

**Annotation:**
```json
{
  "text": "eeh",
  "pos": "INTJ",
  "lemma": "eeh",
  "dep": "",
  "head_text": "",
  "morph": {}
}
```

---

## Idempotenz & Modi

### Safe-Modus (Standard)

**Ablauf:**
1. Lade Datei
2. Pr√ºfe `ann_meta.version`
3. Berechne `text_hash`
4. Pr√ºfe Required Fields
5. **√úberspringe** wenn alles aktuell
6. Sonst: Annotiere neu

**Kommando:**
```bash
python annotation_json_in_media_v2.py safe
```

### Force-Modus

**Ablauf:**
1. Ignoriere Idempotenz-Check
2. Annotiere **alle** Dateien neu

**Kommando:**
```bash
python annotation_json_in_media_v2.py force
```

### Logging

Pro Lauf werden geloggt:
- ‚úÖ Anzahl √ºbersprungene Dateien (+ Grund)
- ‚úÖ Anzahl neu annotierte Dateien
- ‚úÖ Fehlende Felder vor/nach Lauf
- ‚úÖ Zeitformen-Statistiken (Sample)

---

## Validierung & Smoke-Tests

### Erwartete Ergebnisse

| Kontext | Token | Erwartetes Label | Feld |
|---------|-------|------------------|------|
| `"ya ha cantado"` | `"cantado"` | `PerfectoCompuesto` | `past_type` |
| `"hab√≠a cantado"` | `"cantado"` | `Pluscuamperfecto` | `past_type` |
| `"habr√° cantado"` | `"cantado"` | `FuturoPerfecto` | `past_type` |
| `"habr√≠a cantado"` | `"cantado"` | `CondicionalPerfecto` | `past_type` |
| `"cant√©"` | `"cant√©"` | `PerfectoSimple` | `past_type` |
| `"no vamos a cantar"` | `"cantar"` | `analyticalFuture` | `future_type` |
| `"iba a cantar"` | `"cantar"` | `analyticalFuture_past` | `future_type` |
| `"ir a Madrid"` | `"ir"` | `""` (kein Label) | `future_type` |
| `"hubo lluvia"` | `"hubo"` | `""` (kein Label) | `past_type` |

### Pr√ºf-Query (nach Annotation)

**In Python:**
```python
import json

with open("media/transcripts/ARG/001.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Pr√ºfe Metadaten
assert data["ann_meta"]["version"] == "corapan-ann/v2"

# Pr√ºfe Token-Felder
for seg in data["segments"]:
    for w in seg["words"]:
        assert "token_id" in w
        assert "sentence_id" in w
        assert "past_type" in w  # Kann leer sein
        assert "future_type" in w
        assert "norm" in w
```

### Statistik-Pr√ºfung

Nach Lauf wird automatisch geloggt:
```
üìä ZEITFORMEN-STATISTIKEN
   üïê Vergangenheitsformen:
      ‚Ä¢ PerfectoCompuesto        1,234 (2.45%)
      ‚Ä¢ PerfectoSimple          3,456 (6.89%)
      ‚Ä¢ Pluscuamperfecto          234 (0.47%)
   
   üïë Zukunftsformen:
      ‚Ä¢ analyticalFuture          567 (1.13%)
      ‚Ä¢ analyticalFuture_past      89 (0.18%)
```

---

## Performance

### Laufzeit

**Benchmark** (typische Datei mit 3.500 Tokens):
- **v1 (String-basiert):** ~45 Sekunden
- **v2 (Lemma-basiert):** ~48 Sekunden (+7%)

**Overhead durch Gap-Handling:** Minimal (<10%)

### Speicher

**JSON-Dateigr√∂√üe:**
- **v1:** ~150 KB (ohne IDs/norm)
- **v2:** ~220 KB (mit IDs/norm/flachen Feldern) (+47%)

### Optimierungen

- ‚úÖ Idempotenz verhindert unn√∂tige Re-Annotationen
- ‚úÖ Gap-Search limitiert auf ¬±3 Tokens
- ‚úÖ Lokale Suche pro Satz (nicht global)

---

## Migration von v1 zu v2

### Automatische Migration

**Script f√ºhrt automatisch durch:**
1. Entfernt alte Annotations-Felder (au√üer text/start/end)
2. Generiert IDs
3. F√ºhrt spaCy-Annotation aus
4. Post-Processing (Zeitformen)
5. Flatten f√ºr BlackLab
6. Schreibt `ann_meta`

### Kompatibilit√§t

**v1-Dateien:**
- ‚úÖ Werden erkannt (fehlende `ann_meta.version`)
- ‚úÖ Automatisch auf v2 migriert
- ‚ö†Ô∏è Alte Felder werden √ºberschrieben

**Backup empfohlen vor Migration!**

---

## Fehlerbehandlung

### H√§ufige Fehler

| Fehler | Ursache | L√∂sung |
|--------|---------|--------|
| `"PRESENT_FORMS" is not defined` | Alte v1-Imports | Nutze v2-Script |
| `Missing ann_meta` | v1-Datei | Normal, wird migriert |
| `text_hash mismatch` | Content ge√§ndert | Normal, wird neu annotiert |
| `spaCy model not found` | Modell nicht installiert | `python -m spacy download es_dep_news_trf` |

### Fallback-Strategie

**Token-Matching fehlgeschlagen:**
1. Versuche vorw√§rts-Suche in spaCy-Doc
2. Falls nicht gefunden: Parse Token einzeln (`annotate_fallback`)
3. Setze minimale Annotation (pos/lemma/morph)

---

## Siehe auch

- [How-To: JSON Annotation Workflow](../how-to/json-annotation-workflow.md) - Praktische Anleitung
- [Corpus Search Architecture](corpus-search-architecture.md) - Such-Backend Integration
- [Database Creation](../operations/database-creation.md) - DB-Import nach Annotation
- [spaCy Documentation](https://spacy.io/models/es) - Spanische Modelle
- [Universal Dependencies](https://universaldependencies.org/) - POS/Morph-Tags
